{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pytorch for NLP1\n",
    "\n",
    "This notebook is meant to give a short introduction to Pytorch basics.\n",
    "\n",
    "**You do not have to hand in this tutorial.** It is just to help you get started with the projects.\n",
    "\n",
    "We assume that you have pytorch installed with Python 3. See http://www.pytorch.org for instructions on how to install it.\n",
    "\n",
    "First, let's check that we can import torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Seed\n",
    "\n",
    "We can set a seed so that we get the same random values each time we re-run the notebook.\n",
    "We will use seed [42](https://goo.gl/S3wrAV) here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb17407b348>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are the Torch equivalent of **Numpy arrays**, but crucially, they can also be used on a GPU (Video card). This can make your calculations a lot faster.\n",
    "\n",
    "Since they are so similar, you can actually **convert** most tensors to Numpy arrays (and back), but we won't need to do that so often.\n",
    "\n",
    "Working with PyTorch, we will need lots of tensors of various shapes.\n",
    "For example, if we want to transform an input vector $\\mathbf{x}$, we will need a weight matrix $W$.\n",
    "\n",
    "**Note:** \"Tensor\" is a general name. A 1-D tensor is also called a **vector**, a 2-D tensor a **matrix**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(0 ,.,.) = \n",
      " -2.1696e+13  4.5577e-41 -2.1696e+13\n",
      "  4.5577e-41  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.FloatTensor of size 2x3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create uninitialized 3-D tensor (values can be anything that is in memory!)\n",
    "x = torch.Tensor(2, 3, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2438  0.5595  0.6053\n",
      " 0.1552  0.8602  0.9004\n",
      " 0.6183  0.3922  0.9343\n",
      " 0.9698  0.6690  0.1418\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a randomly initialized 2-D tensor (a matrix)\n",
    "x = torch.rand(4, 3)\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# how to get its size\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n"
     ]
    }
   ],
   "source": [
    "# or if you know there are 2 dimensions:\n",
    "time, dim = x.size()\n",
    "print(time, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations\n",
    "\n",
    "#### Adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2476  0.8941  0.8603\n",
      " 0.7311  1.2349  1.0261\n",
      " 0.7646  1.1235  1.8144\n",
      " 0.9805  1.6034  0.8417\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      " 0.2476  0.8941  0.8603\n",
      " 0.7311  1.2349  1.0261\n",
      " 0.7646  1.1235  1.8144\n",
      " 0.9805  1.6034  0.8417\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      " 0.2476  0.8941  0.8603\n",
      " 0.7311  1.2349  1.0261\n",
      " 0.7646  1.1235  1.8144\n",
      " 0.9805  1.6034  0.8417\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      " 0.2476  0.8941  0.8603\n",
      " 0.7311  1.2349  1.0261\n",
      " 0.7646  1.1235  1.8144\n",
      " 0.9805  1.6034  0.8417\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can add tensors in many ways. \n",
    "# The easiest is to simply use a python + operator:\n",
    "y = torch.rand(4, 3)\n",
    "print(x + y)\n",
    "\n",
    "# But you can also use torch.add:\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# Provide an output Tensor and save the result there:\n",
    "result = torch.Tensor(4, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "\n",
    "# Or add in-place (this changes y!)\n",
    "# Note: Any operation that mutates a tensor in-place is post-fixed with an \"_\", like \"add_\" here.\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix multiplication\n",
    "\n",
    "Matrix multiplications are essential for Neural networks. Quite often, we have an input vector $\\mathbf{x}$, and then we want to learn weights $W$ that transform that input to some output that we want. \n",
    "\n",
    "We will now walk you through matrix multiplication in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Let's create a vector x with values 0..5\n",
    "# We can use the arange function for that:\n",
    "x = torch.arange(0, 6)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  1  2\n",
      " 3  4  5\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Now, we will reshape x to have shape 2x3\n",
    "# That is, it will become a matrix!\n",
    "# The values will be the same, we will just look at them differently.\n",
    "x = x.view((2, 3))\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  1  2\n",
      " 3  4  5\n",
      " 6  7  8\n",
      "[torch.FloatTensor of size 3x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now, let's create a square matrix W:\n",
    "W = torch.arange(0, 9).view((3, 3))\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 15  18  21\n",
      " 42  54  66\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we can perform matrix multiplication, since we have 2x3 and 3x3 matrices!\n",
    "# Verify if you can do this multiplication by hand, too!\n",
    "# If you need some help, you can check here: https://www.mathsisfun.com/algebra/matrix-multiplying.html\n",
    "h = torch.matmul(x, W)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More operations\n",
    "In case you want to do something different from addition or matrix-multiplying (and that is quite likely!), you can read here about all of Torch's operations: http://pytorch.org/docs/master/torch.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "It is quite common that we need to select a part of a tensor. Indexing works just like in Numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.8941\n",
      " 1.2349\n",
      " 1.1235\n",
      " 1.6034\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "\n",
      " 0.2476\n",
      " 0.8941\n",
      " 0.8603\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "\n",
      " 0.8603\n",
      " 1.0261\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "\n",
      " 0.7311  1.2349  1.0261\n",
      " 0.7646  1.1235  1.8144\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(result[:, 1])    # second column\n",
    "print(result[0])       # first row\n",
    "print(result[:2, -1])  # first two rows, last column\n",
    "print(result[1:3, :])  # middle two rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation with Autograd\n",
    "\n",
    "One of the main reasons for using PyTorch is that we can automatically get **gradients/derivatives** of functions that we define. We will mainly use PyTorch for using Neural networks, and they are just fancy functions! If we use weight matrices in our function that we want to learn, then those are called the **parameters** or simply the **weights**.\n",
    "\n",
    "If our Neural Network would output a single scalar value, we would talk about taking the **derivative**, but you will see that quite often we will have **multiple** output variables (\"values\"); in that case we talk about **gradients**. It's a more general term.\n",
    "\n",
    "Given an input $\\mathbf{x}$, we define our function by **manipulating** that input, usually by matrix-multiplications with weight matrices and additions with so-called bias vectors. As we manipulate our input, we are automatically creating a **computational graph**. This graph shows how to arrive at our output from our input. \n",
    "PyTorch is a **define-by-run** framework; this means that we can just do our manipulations, and PyTorch will keep track of that graph for us!\n",
    "\n",
    "So, to recap: the only thing we have to do is to compute the **output**, and then we can ask PyTorch to automatically get the **gradients**. \n",
    "\n",
    "> **Note:  Why do we want gradients?** Consider that we have defined a function, a Neural Net, that is supposed to compute a certain output $y$ for an input vector $\\mathbf{x}$. We then define an **error measure** that tells us how wrong our network is; how bad it is in predicting output $y$ from input $\\mathbf{x}$. Based on this error measure, we can use the gradients to **update** the weights $W$ that were responsible for the output, so that the next time we present input $\\mathbf{x}$ to our network, the output will be closer to what we want. \n",
    "\n",
    "### Variable\n",
    "\n",
    "In order to get the autograd functionality, we will need to wrap all our Tensors in Variables. You can import the Variable class like this:\n",
    "\n",
    "```python\n",
    "from torch.autograd import Variable\n",
    "```\n",
    "\n",
    "Converting a Tensor to a Variable is easy:\n",
    "\n",
    "```python\n",
    "# example\n",
    "x = torch.ones(2, 2)\n",
    "x = Variable(x, requires_grad=True)\n",
    "```\n",
    "\n",
    "You can still access the underlying Tensor using `.data`:\n",
    "\n",
    "```python\n",
    "print(x.data)\n",
    "```\n",
    "\n",
    "Let's try it out!\n",
    "\n",
    "### Example\n",
    "\n",
    "We're going to define a function $$y_i = (x_i + 2)^2 + 3$$\n",
    "And as our final output $o$ we take the mean over all values $y_i$, so we get a single output value:\n",
    "\n",
    "$$o = \\frac{1}{|y|} \\sum_i y_i$$\n",
    "\n",
    "As our input $\\mathbf{x}$ we'll use a vector with 3 values: $[1, 1, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# create an input vector x\n",
    "x = Variable(torch.ones(3), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 12\n",
      " 12\n",
      " 12\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we define our function\n",
    "# Note that, even though x is a vector, we can still add a single value to it.\n",
    "# PyTorch will just add that value to each element of the vector.\n",
    "y = (x + 2)**2 + 3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.AddConstantBackward object at 0x7fb3ebe3ed68>\n"
     ]
    }
   ],
   "source": [
    "# y has a grad_fn since it was created by an operation\n",
    "# this grad_fn will be used by PyTorch for obtaining the gradient\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 12\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Our final output o is the mean\n",
    "o = y.mean()\n",
    "print(o) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can take the gradients by calling o.backward()\n",
    "# this will populate x.grad\n",
    "o.backward() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x.grad` will now contain the gradient $\\partial o/ \\partial x$, and this will say how a change in $x$ will affect output $o$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying the gradient by hand\n",
    "\n",
    "You should now see a gradient of `[2, 2, 2]`.\n",
    "\n",
    "We can verify this by hand!\n",
    "\n",
    "Our output $\\mathbf{o}$ is the mean of $\\mathbf{y}$:\n",
    "$$o = \\frac{1}{3}\\sum_i y_i$$\n",
    "\n",
    "And $\\mathbf{y}$ consists of elements $y_i$:\n",
    "\n",
    "$$y_i = (x_i+2)^2 + 3$$ \n",
    "\n",
    "We know that $y_i = 12$, given that $x_i = 1$ (for each $i$):\n",
    "$$y_i\\bigr\\rvert_{x_i=1} = 12$$\n",
    "\n",
    "Therefore,\n",
    "$$\\frac{\\partial o}{\\partial x_i} = \\frac{\\partial o}{\\partial y_i}\\frac{\\partial y_i}{\\partial x_i} = \\underbrace{\\frac{1}{3}}_{\\frac{\\partial o}{\\partial y_i}} \\underbrace{2 (x_i+2)}_{\\frac{\\partial y_i}{\\partial x_i}} = \\frac{2}{3} (x_i+2)$$\n",
    "\n",
    "hence\n",
    "$$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{2}{3} * 3 = 2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN module\n",
    "\n",
    "Now if we want to build a big Neural Net, we could specify all our parameters (weight matrices, bias vectors) using `Variables`, ask PyTorch to calculate the gradients and then adjust the parameters. But things can quickly get cumbersome if we have a lot of parameters. In PyTorch, there is a package called `torch.nn` that makes building Neural Nets more convenient. \n",
    "\n",
    "Let's define a very simple Neural Net to show you how it works. The network performs a **logistic regression**, i.e. it calculates:\n",
    "$$ y = \\sigma( W \\mathbf{x} + b )$$\n",
    "\n",
    "You have already seen how to calculate $W \\mathbf{x} + b$; it's a matrix multiplication with an added bias. The function $\\sigma$ might be new: it is the sigmoid function, and it is defined as:\n",
    "$$ \\sigma(x) = \\frac{1}{1+ \\exp(-x)} $$\n",
    "\n",
    "The $\\exp$ makes sure all values are positive, while the rest scales them between 0 and 1. \n",
    "You can see the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb449f2f4e0>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3eb2ca5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "x = np.arange(-10., 10., 0.2)\n",
    "plt.plot(x, sigmoid(x), 'b', label='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, we are often doing **classification**, rather than regression (predicting a value). So, even though the name can be misleading, logistic regression is a **classifier**: we have two output classes 0 and 1.\n",
    "\n",
    "To get the 2 classes, we use the sigmoid function $\\sigma$, so that the values coming out of our NN are between 0 and 1. You can see that in the picture above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExampleNN (\n",
      "  (linear): Linear (3 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ExampleNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ExampleNN, self).__init__()\n",
    "        \n",
    "        # an affine operation: y = Wx + b\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = F.sigmoid(x)  # output values are squashed between 0 and 1\n",
    "        return x\n",
    "\n",
    "net = ExampleNN()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this is a very simple Neural Network! Actually, it is so simple maybe we should not call it a Neural network. But let's do so anyway.\n",
    "\n",
    "This is what you should know: \n",
    "\n",
    "- when defining your Neural Net, you create a class that *inherits* from `nn.Module`. \n",
    "- We called our Neural Net `ExampleNN`.\n",
    "- Parameters are defined within the `__init__` method. Here we defined a single **linear** layer. The parameters for that layer (a weight matrix $W$ and a bias term $b$) are added **automatically** to our parameter list.\n",
    "- We define our computation in the `forward` function. In this case, we apply the linear layer to our input $\\mathbf{x}$ and then a sigmoid function.\n",
    "\n",
    "Let's check if our parameters are indeed as we expect them to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-0.3495 -0.5127 -0.3329\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      " -5.4444\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "\n",
    "for p in params:\n",
    "    print(p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that:\n",
    "\n",
    "- the first parameter is our weight matrix $W$, which is shaped $[1, 3]$.\n",
    "- the second parameter is the bias term $b$, which is a scalar (a single value), since the weight matrix transforms our input $\\mathbf{x}$ (with 3 elements) into a single scalar.\n",
    "\n",
    "Observe that these parameters have been randomly initialized.\n",
    "\n",
    "Now that we have our NN, we can feed it an input and see what comes out.\n",
    "The input has to be a `Variable`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Variable containing:\n",
      " 1.2545\n",
      " 0.4622\n",
      "-0.5009\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "output: Variable containing:\n",
      " 0.3628\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(3))\n",
    "print(\"input:\", x)\n",
    "out = net(x)\n",
    "print(\"output:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "Now we would like to use the `autograd` functionality to get gradients, but we first need a loss!\n",
    "The loss will tell us how well our network is doing.\n",
    "\n",
    "We're going to say that, for our input example, the **target** value is `0`. \n",
    "The target is what we wanted our network to **predict** for the input that we gave.\n",
    "\n",
    "As our **loss** (or \"criterion\") we'll use the MSE, Mean Squared Error:\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (o_i - t_i)^2 $$\n",
    "\n",
    "I.e. it is the average, over elements $i$, of the squared difference of output $o_i$ with target $t_i$.\n",
    "Since we have a single output value here, our loss is simply $(o - t)^2$.\n",
    "\n",
    "Let's calculate our loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: Variable containing:\n",
      " 0.3628\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "loss: Variable containing:\n",
      " 0.1316\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out = net(x)\n",
    "target = Variable(torch.zeros(1))  # a dummy target (0.)\n",
    "criterion = nn.MSELoss()  # this is our criterion\n",
    "loss = criterion(out, target)\n",
    "print(\"output:\", out)\n",
    "print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll ask PyTorch to **update** the weights (parameters) of our neural network so that our next prediction is closer to that target.\n",
    "\n",
    "We first need to zero-out all gradient tensors. `net.zero_grad()` will do this for all parameters. It will set `p.grad` to zeros for each parameter $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()  # reset gradients\n",
    "loss.backward()  # compute gradients\n",
    "\n",
    "# update weights\n",
    "learning_rate = 0.5\n",
    "for f in net.parameters():\n",
    "    # for each parameter, take a small step in the opposite dir of the gradient\n",
    "    # sub_ substracts in-place\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we check the output for the same input vector $\\mathbf{x}$, the output should be closer to the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: Variable containing:\n",
      " 0\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "out: Variable containing:\n",
      " 0.3628\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "new out (should be closer to target): Variable containing:\n",
      " 0.3062\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "\n",
      "loss: Variable containing:\n",
      " 0.1316\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "new loss (should be lower): Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.3753\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_out = net(x)\n",
    "new_loss = criterion(new_out, target)\n",
    "\n",
    "print(\"target:\", target)\n",
    "print(\"out:\", out)\n",
    "print(\"new out (should be closer to target):\", new_out)\n",
    "\n",
    "print(\"\\nloss:\", loss)\n",
    "print(\"new loss (should be lower):\", new_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BOW) classifier\n",
    "\n",
    "Congratulations, that's the basics! \n",
    "\n",
    "\n",
    "We will now take a look at the BOW model that we showed in the beginning. \n",
    "Given a sentence (a sequence of tokens), we will predict the **sentiment** of that sentence (0 - very negative, ..., 4 - very positive). \n",
    "\n",
    "We associate a learned vector $\\mathbf{w}$ (size $5$) with every token. \n",
    "To determine the sentiment, we will add up the word vectors, also called the **embeddings**, of each token in the sentence. We also add a **bias vector**, which is also a learned vector of size 5.\n",
    "\n",
    "You can now take a look at `bow-simple.py`.  You will see that the bias vector is missing.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Always take a look at the **data** first! Open `data/classes/test.txt` in your favorite editor. On Linux/OS X, use `less filename` from the Terminal to open big files without filling your memory.\n",
    "\n",
    "2. Take a look at how the data is *read in* and how the *dictionaries* are created. Make sure you understand how this works. The dictionaries convert words to integers, and this will be very useful when we want to sum the embeddings of certain words in a sentence.\n",
    "\n",
    "3. Find the location where the Variable is defined that holds the weights for each token. On a separate line, add another Variable that just holds the **bias** (a vector of size `ntags`). Like the weights, this vector should be initialized randomly.\n",
    "\n",
    "4. Find the `calc_scores` function and make sure you understand what it does. Then, add the bias parameter to the `score`, before it is returned.\n",
    "\n",
    "5. Make sure the bias parameter is also updated after the gradients are calculated. The bias term is updated similarly to how the weights are updated, after `output.backward()` is called.\n",
    "\n",
    "6. Finally, don't forget to zero-out the gradient tensor of the bias parameters after updating. Again, this is similar to the code that zeros out the gradients for the weights.\n",
    "\n",
    "Now, play a bit with your BOW classifier. Does it work well?\n",
    "\n",
    "\n",
    "## BOW using torch.nn\n",
    "\n",
    "In the `bow-simple.py` code we did a lot of stuff manually; the updates for example.\n",
    "Now take a look at `bow-nn.py`. In this code we use the `torch.nn` package to use some higher-level functions.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. The bias term is missing again; please add it back. This time, you will have to define it in the `__init__` method of the BOW model. Hint: you will need to use `nn.Parameter`.\n",
    "\n",
    "2. In the `forward` function of the model, make sure to sum the bias that you defined in the previous step.\n",
    "\n",
    "3. Now take a look at how the weights are updated. You don't have to change this code, since the optimizer automatically finds all the model parameters (w and b). This code uses the `torch.optim` package to do the same as what we did manually before.\n",
    "\n",
    "Make sure that you can run this code, and verify that it gives similar results to the manual code.\n",
    "\n",
    "\n",
    "## Continious BOW (CBOW) classifier\n",
    "\n",
    "Now that you've looked at the BOW models, it is time to take a look at `cbow.py`.\n",
    "The difference with BOW is that we can now assign more parameters per token, and this allows us to capture more fine-grained aspects of the tokens. Before, we could only associate how much a token was associated with each class.\n",
    "But now we can use as many dimensions as we like. One dimension could be if the word is an adjective, or if it describes an object. We will now use a separate weight matrix to transform the sum of word representations into the number of classes that we have:\n",
    "$$o = W \\mathbf{x} + b$$\n",
    "where $\\mathbf{x}$ is the sum of the word embeddings.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. In the `__init__` method of the model, add a `nn.Linear` layer to transform a vector of `embedding_dim` into a vector of size `ntags`. Note that the linear layer \"secretly\" defines paramters $W$ and $b$ for us.\n",
    "\n",
    "2. In the `forward` method, apply the `bow` (the sum of word embeddings) to the linear layer, like this: `logits = self.linear(bow)`\n",
    "\n",
    "3. Now take a look at the training loop. Did you notice that `lookup_tensor` takes as input a list of sentences? Why is this?\n",
    "\n",
    "You can now play around with this code. You can e.g. try different learning rates, and adding more layers. \n",
    "\n",
    "## That's it! \n",
    "\n",
    "We will release the solutions next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
